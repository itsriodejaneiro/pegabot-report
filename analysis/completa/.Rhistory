filter(Resultado == "Alta") %>%
distinct(user_id, `username`, `Análise Total`)
# Filtrar tweets de bots
bots_twint <- full_data %>%
filter(user_id %in% bots$user_id)
# Perfis que não puderam ser analisados
erro <- full_data %>%
filter(is.na(`Análise Total`)) %>%
distinct(user_id, username_original)
erros_twint <- full_data %>%
filter(user_id %in% erro$user_id)
# Números gerais
ntweets_analisados <- length(unique(full_data$id)); ntweets_analisados
nperfis_total <- length(unique(full_data$user_id)); nperfis_total
nperfis_analisados <- full_data %>%
select('username') %>%
drop_na() %>%
distinct() %>%
count(); nperfis_analisados
nerros <- nrow(erro); nerros
percent_erros <- percent(nerros/nperfis_analisados$n[1], 0.1); percent_erros
nposts_erro <- nrow(erros_twint); nposts_erro
percent_posts_erro <- percent(nposts_erro/ntweets_analisados); percent_posts_erro
primeiro_dia <- min(full_data$date); primeiro_dia
ultimo_dia <- max(full_data$date); ultimo_dia
nbots <- length(unique(bots$user_id)); nbots
nbots_twint <- length(unique(bots_twint$user_id)); nbots_twint
percent_bots <- percent(nbots/nperfis_analisados$n, 0.1); percent_bots
nposts_bots <- nrow(bots_twint); nposts_bots
ntweets_bots_twint <- bots_twint %>% filter(retweet == 'False') %>% nrow(); ntweets_bots_twint
nretweets_bots_twint <- bots_twint %>% filter(retweet == 'True') %>% nrow(); nretweets_bots_twint
percent_posts_bots <- percent(nposts_bots/ntweets_analisados, 0.1); percent_posts_bots
percent_tweets_bots <- percent(ntweets_bots_twint/nposts_bots); percent_tweets_bots
percent_retweets_bots <- percent(nretweets_bots_twint/nposts_bots); percent_retweets_bots
total_horas <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "h")),); total_horas
total_dias <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "d")),); total_dias
total_semanas <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "w")),); total_semanas
stats_df <- t(data.frame('Total de tweets' = ntweets_analisados,
'Total_perfis analisados' = nperfis_analisados,
'Total perfis não analisados' = nerros,
'Porc. perfis com erro' = percent_erros,
'Inicio' = primeiro_dia,
'Fim' = ultimo_dia,
'Total de horas' = total_horas,
'Total de dias' = total_dias,
'Total de semanas' = total_semanas,
'Total perfis automatizados' = nbots,
'Porc. Total perfis automatizados' = percent_bots,
'Posts de perfis automatizados' = nposts_bots,
'Porc. Posts de perfis automatizados' = percent_posts_bots))
output_file <- "resumo.xlsx"
# Salvar arquivo
write.xlsx2(stats_df, output_file, sheetName = "Resumo",
col.names = F, row.names = TRUE, append = TRUE)
nretweets <- nrow(full_data[full_data$retweet == 'True',]); nretweets
ntweets <- nrow(full_data[full_data$retweet == 'False',]); ntweets
percent_rts <- percent(nretweets/ntweets_analisados, 0.1); percent_rts
percent_tweets <- percent(ntweets/ntweets_analisados, 0.1); percent_tweets
# Plot
tweets_rts <- c(`RT` = nretweets, `Tweets` = ntweets)
waffle(title = 'Proporção de RTs e Tweets compartilhados pelos usuários',
tweets_rts / 1000, rows = 6,
colors = c(paleta_its[2], paleta_its[6]),
xlab = 'cada unidade = 10000 registros')
# Load packages
source('src/initialConfig.R')
# Plot
tweets_rts <- c(`RT` = nretweets, `Tweets` = ntweets)
waffle(title = 'Proporção de RTs e Tweets compartilhados pelos usuários',
tweets_rts / 1000, rows = 6,
colors = c(paleta_its[2], paleta_its[6]),
xlab = 'cada unidade = 10000 registros')
# Load packages
source('src/initialConfig.R')
## 1. Leitura dos resultados do pegabot #####
base_pegabot <- read_csv("data/processed/base_pegabot.csv",
col_types = cols(.default = "c"))
# Ajustes no username to lower e com @
base_pegabot$`Perfil Twitter` <- gsub('@', '', tolower(base_pegabot$`Perfil Twitter`))
base_pegabot$id_pegabot <- seq(1:nrow(base_pegabot))
## 2. Leitura da base twint #####
base_twint <- read_csv("data/processed/base_twint.csv",
col_types = cols(.default = "c"))
length(unique(base_twint$id))
length(unique(base_twint$user_id_str))
# Ajustes no username de base_twint to lower e com @
base_twint$username_original <- base_twint$username
base_twint$username <- gsub('@', '', tolower(base_twint$username))
write_csv(base_twint_cleaned, 'data/processed/base_twint.csv')
base_twint <- read_csv('Dados/base_twint.csv', col_types = cols(.default = "c"))
## 3. Juntar tudo #####
#bp <- base_pegabot; bp$id_usuario_pb <- bp$`ID do Usuário`#bp$`ID do Usuário` <- as.numeric(bp$`ID do Usuário`)
#twint <- base_twint; #twint$user_id <- as.numeric(twint$user_id)
full_data <- merge(base_twint, base_pegabot, by.x = 'username', by.y = 'Perfil Twitter', all.x = T); #rm(bp); rm(twint)
length(unique(full_data$user_id))
full_data <- full_data[!duplicated(full_data$id, ), ]
# Manter só colunas de interesse
colnames(full_data)
cols_keep <- colnames(full_data)[c(1,4:8,10,13,14,17,18,22:27,31:35,40:52)]
full_data <- full_data %>%
select(cols_keep); rm(cols_keep)
full_data$user_id <- as.character(full_data$user_id)
# Remover duplicadas
#full_data_cleaned <- full_data[order(all$id.x, -abs(as.numeric(all$nretweets))), ]
#full_data_cleaned <- full_data_cleaned[ !duplicated(full_data_cleaned$id.x),]
#full_data <- full_data_cleaned; rm(full_data_cleaned)
length(unique(full_data$id))
length(unique(full_data$user_id))
# Porcentagem de 70%+ na lista da hashtag
full_data <- full_data %>% # Separando a base
mutate(Resultado = cut(x = as.numeric(`Análise Total`),
breaks = c(0, 70, Inf),
labels = c("Baixa",
"Alta")))
# Para agilizar:
write_csv(full_data, 'data/processed/full_data.csv')
full_data <- read_csv('data/processed/full_data.csv', col_types = cols(.default = "c"))
bots <- full_data %>%
filter(Resultado == "Alta") %>%
distinct(user_id, `username`, `Análise Total`)
# Filtrar tweets de bots
bots_twint <- full_data %>%
filter(user_id %in% bots$user_id)
# Perfis que não puderam ser analisados
erro <- full_data %>%
filter(is.na(`Análise Total`)) %>%
distinct(user_id, username_original)
erros_twint <- full_data %>%
filter(user_id %in% erro$user_id)
# Números gerais
ntweets_analisados <- length(unique(full_data$id)); ntweets_analisados
nperfis_total <- length(unique(full_data$user_id)); nperfis_total
nperfis_analisados <- full_data %>%
select('username') %>%
drop_na() %>%
distinct() %>%
count(); nperfis_analisados
nerros <- nrow(erro); nerros
percent_erros <- percent(nerros/nperfis_analisados$n[1], 0.1); percent_erros
nposts_erro <- nrow(erros_twint); nposts_erro
percent_posts_erro <- percent(nposts_erro/ntweets_analisados); percent_posts_erro
primeiro_dia <- min(full_data$date); primeiro_dia
ultimo_dia <- max(full_data$date); ultimo_dia
nbots <- length(unique(bots$user_id)); nbots
nbots_twint <- length(unique(bots_twint$user_id)); nbots_twint
percent_bots <- percent(nbots/nperfis_analisados$n, 0.1); percent_bots
nposts_bots <- nrow(bots_twint); nposts_bots
ntweets_bots_twint <- bots_twint %>% filter(retweet == 'False') %>% nrow(); ntweets_bots_twint
nretweets_bots_twint <- bots_twint %>% filter(retweet == 'True') %>% nrow(); nretweets_bots_twint
percent_posts_bots <- percent(nposts_bots/ntweets_analisados, 0.1); percent_posts_bots
percent_tweets_bots <- percent(ntweets_bots_twint/nposts_bots); percent_tweets_bots
percent_retweets_bots <- percent(nretweets_bots_twint/nposts_bots); percent_retweets_bots
total_horas <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "h")),); total_horas
total_dias <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "d")),); total_dias
total_semanas <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "w")),); total_semanas
stats_df <- t(data.frame('Total de tweets' = ntweets_analisados,
'Total_perfis analisados' = nperfis_analisados,
'Total perfis não analisados' = nerros,
'Porc. perfis com erro' = percent_erros,
'Inicio' = primeiro_dia,
'Fim' = ultimo_dia,
'Total de horas' = total_horas,
'Total de dias' = total_dias,
'Total de semanas' = total_semanas,
'Total perfis automatizados' = nbots,
'Porc. Total perfis automatizados' = percent_bots,
'Posts de perfis automatizados' = nposts_bots,
'Porc. Posts de perfis automatizados' = percent_posts_bots))
output_file <- "resumo.xlsx"
# Salvar arquivo
write.xlsx2(stats_df, output_file, sheetName = "Resumo",
col.names = F, row.names = TRUE, append = TRUE)
ntweets <- nrow(full_data[full_data$retweet == 'False',]); ntweets
percent_rts <- percent(nretweets/ntweets_analisados, 0.1); percent_rts
percent_tweets <- percent(ntweets/ntweets_analisados, 0.1); percent_tweets
nretweets <- nrow(full_data[full_data$retweet == 'True',]); nretweets
ntweets <- nrow(full_data[full_data$retweet == 'False',]); ntweets
percent_rts <- percent(nretweets/ntweets_analisados, 0.1); percent_rts
percent_tweets <- percent(ntweets/ntweets_analisados, 0.1); percent_tweets
# Plot
tweets_rts <- c(`RT` = nretweets, `Tweets` = ntweets)
waffle(title = 'Proporção de RTs e Tweets compartilhados pelos usuários',
tweets_rts / 1000, rows = 6,
colors = c(paleta_its[2], paleta_its[6]),
xlab = 'cada unidade = 10000 registros')
plot_tweets_rts <- waffle(
tweets_rts / 1000, rows = 6,
colors = c(paleta_its[2], paleta_its[6]),
#use_glyph = 'twitter',
#glyph_size = 7,
#glyph_font = "Font Awesome 5 Brands Regular",
#glyph_font_family = "FontAwesome5Brands-Regular",
legend_pos="bottom-left") +
labs(title = "Proporção de <b style='color:#8a8a8a'>tweets</b> e <b style='color:#54318C'>RTs</b> ",
subtitle = "<span style='color:#6c6c6c'>cada unidade = 1.000 registros</span>") +
theme(text = element_text(family = "sans"),
plot.title = element_markdown(lineheight = 1.1, size = 20),
plot.subtitle = element_markdown(lineheight = 1.1, size = 16),
axis.title.x = element_text(size = 13, vjust = 1,
hjust = 0, colour = paleta_its[7]),
plot.margin = unit(c(0,0,0,0),"mm")
); plot_tweets_rts
ggsave('output/plot0_tweets_rts.png', width = 7, height = 5, units = 'in', dpi = 100)
## 3. Analise Temporal #####
full_data_bckp <- full_data
# Adicionar intervalo de 7 dias
full_data$interval_7day <- cut(as.POSIXct(full_data$date), breaks = "7 days",)
# Adicionar intervalo de 1 dia
full_data$interval_1day <- cut(as.POSIXct(full_data$date), breaks = "1 day",)
# Adicionar intervalo de 1 h
full_data$interval_1hour <- cut(as.POSIXct(full_data$date), breaks = "1 hour",)
# Adicionar intervalo de 2 h
full_data$interval_2hour <- cut(as.POSIXct(full_data$date), breaks = "2 hour",)
# Adicionar intervalo de 6 h
full_data$interval_6hour <- cut(as.POSIXct(full_data$date), breaks = "6 hour",)
# Adicionar intervalo de 3 h
full_data$interval_3hour <- cut(as.POSIXct(full_data$date), breaks = "3 hour",)
# Adicionar intervalo de 10 min
full_data$interval_10min <- cut(as.POSIXct(full_data$date), breaks = "10 min",)
# Adicionar intervalo de 5 min
full_data$interval_5min <- cut(as.POSIXct(full_data$date), breaks = "5 min")
# Para agilizar 2:
write_csv(full_data, 'data/processed/full_data_details.csv')
full_data <- read_csv('data/processed/full_data_details.csv', col_types = cols(.default = "c"))
# Por dia
ft_temporal <- plyr::count(full_data, 'interval_3hour')
ft_temporal$interval <- format(strptime(ft_temporal$interval_3hour,
"%Y-%m-%d %H:%M"),
format = "%d/%m (%Hh)")
ft_temporal_outline <- boxplot.stats(ft_temporal$freq)$out
peaks <- ft_temporal %>%
filter(freq %in% ft_temporal_outline); peaks; str(peaks) #%>% droplevels()
# Plot temporal
plot_temporal <- ggplot(ft_temporal, aes(x = fct_reorder(interval, as.Date(interval_3hour)), y = freq)) +
geom_line(group = 1, color = paleta_its[2], size = 1) +
#geom_point(data = peaks, aes(x = interval, y = freq),
#           color = paleta_its[7], size = 7, alpha = .4) +
labs(title = "Evolução do compartilhamento dos tweets",
subtitle = "publicados entre os dias 23/08 e 02/09 de 2022",
x = "",
y = "") +
scale_x_discrete(guide = guide_axis(check.overlap = TRUE),
breaks = ft_temporal$interval[seq(1, nrow(ft_temporal), by = 4)]) +
theme_minimal() +
scale_y_continuous(breaks = seq(0, max(ft_temporal$freq) + 10000, 2000),
name = "Nº de tweets") +
theme(text = element_text(family = "sans"),
plot.title = element_text(size = 13,
colour = paleta_its[8]),
plot.subtitle = element_text(size = 12,
colour = paleta_its[8]
),
axis.text.x = element_text(size = 12,
colour = paleta_its[7]),
axis.text.y = element_text(size = 12,
colour = paleta_its[7]),
axis.title.y = element_text(size = 12,
colour = paleta_its[7],
vjust = 2,
hjust = 0.98),
plot.margin = unit(c(.5,2,.5,1),"cm"),
panel.grid.minor.y = element_blank()); plot_temporal
ggsave('output/plot1_temporal.png', width = 18, height = 10, units = 'in', dpi = 100)
write.xlsx2(ft_temporal, output_file, sheetName = "EvolucaoTemporal",
col.names = F, row.names = TRUE, append = TRUE)
percent_peaks_tweets <- percent(length(filter_tweets_peaks$id)/ntweets_analisados); percent_peaks_tweets
# Análise dos picos de compartilhamento
filter_tweets_peaks <- full_data %>%
filter(retweet == 'False') %>%
filter(interval_1day == '2022-09-01') %>%
arrange(desc(as.numeric(nretweets)))
percent_peaks_tweets <- percent(length(filter_tweets_peaks$id)/ntweets_analisados); percent_peaks_tweets
#percent_peaks_tweets <- percent(sum(peaks$freq)/ntweets_analisados); percent_peaks_tweets
percent_peaks_days <- percent(nrow(peaks)/total_dias); percent_peaks_days
# Separando o que é só tweet
filter_tweets_peaks_perday <- filter_tweets_peaks %>%
filter(retweet == 'False') %>%
arrange(-as.numeric(nretweets))
# Plot dias de pico ###
peaks$interval_1day <- levels(droplevels(peaks$interval_1day))
View(peaks)
# Plot dias de pico ###
peaks$interval_1day <- levels(droplevels(peaks$interval_1day))
View(peaks)
#percent_peaks_tweets <- percent(sum(peaks$freq)/ntweets_analisados); percent_peaks_tweets
percent_peaks_days <- percent(nrow(peaks)/total_dias); percent_peaks_days
View(peaks)
# Separando o que é só tweet
filter_tweets_peaks_perday <- filter_tweets_peaks %>%
filter(retweet == 'False') %>%
arrange(-as.numeric(nretweets))
# Plot dias de pico ###
peaks$interval_1day <- levels(droplevels(peaks$interval_1day))
View(ft_temporal)
View(peaks)
# Load packages
source('src/initialConfig.R')
## 1. Leitura dos resultados do pegabot #####
base_pegabot <- read_csv("data/processed/base_pegabot.csv",
col_types = cols(.default = "c"))
# Ajustes no username to lower e com @
base_pegabot$`Perfil Twitter` <- gsub('@', '', tolower(base_pegabot$`Perfil Twitter`))
base_pegabot$id_pegabot <- seq(1:nrow(base_pegabot))
## 2. Leitura da base twint #####
base_twint <- read_csv("data/processed/base_twint.csv",
col_types = cols(.default = "c"))
length(unique(base_twint$id))
length(unique(base_twint$user_id_str))
# Ajustes no username de base_twint to lower e com @
base_twint$username_original <- base_twint$username
base_twint$username <- gsub('@', '', tolower(base_twint$username))
write_csv(base_twint_cleaned, 'data/processed/base_twint.csv')
base_twint <- read_csv('Dados/base_twint.csv', col_types = cols(.default = "c"))
## 3. Juntar tudo #####
#bp <- base_pegabot; bp$id_usuario_pb <- bp$`ID do Usuário`#bp$`ID do Usuário` <- as.numeric(bp$`ID do Usuário`)
#twint <- base_twint; #twint$user_id <- as.numeric(twint$user_id)
full_data <- merge(base_twint, base_pegabot, by.x = 'username', by.y = 'Perfil Twitter', all.x = T); #rm(bp); rm(twint)
length(unique(full_data$user_id))
full_data <- full_data[!duplicated(full_data$id, ), ]
# Manter só colunas de interesse
colnames(full_data)
cols_keep <- colnames(full_data)[c(1,4:8,10,13,14,17,18,22:27,31:35,40:52)]
full_data <- full_data %>%
select(cols_keep); rm(cols_keep)
full_data$user_id <- as.character(full_data$user_id)
# Remover duplicadas
#full_data_cleaned <- full_data[order(all$id.x, -abs(as.numeric(all$nretweets))), ]
#full_data_cleaned <- full_data_cleaned[ !duplicated(full_data_cleaned$id.x),]
#full_data <- full_data_cleaned; rm(full_data_cleaned)
length(unique(full_data$id))
length(unique(full_data$user_id))
# Porcentagem de 70%+ na lista da hashtag
full_data <- full_data %>% # Separando a base
mutate(Resultado = cut(x = as.numeric(`Análise Total`),
breaks = c(0, 70, Inf),
labels = c("Baixa",
"Alta")))
# Para agilizar:
write_csv(full_data, 'data/processed/full_data.csv')
full_data <- read_csv('data/processed/full_data.csv', col_types = cols(.default = "c"))
bots <- full_data %>%
filter(Resultado == "Alta") %>%
distinct(user_id, `username`, `Análise Total`)
# Filtrar tweets de bots
bots_twint <- full_data %>%
filter(user_id %in% bots$user_id)
# Perfis que não puderam ser analisados
erro <- full_data %>%
filter(is.na(`Análise Total`)) %>%
distinct(user_id, username_original)
erros_twint <- full_data %>%
filter(user_id %in% erro$user_id)
# Números gerais
ntweets_analisados <- length(unique(full_data$id)); ntweets_analisados
nperfis_total <- length(unique(full_data$user_id)); nperfis_total
nperfis_analisados <- full_data %>%
select('username') %>%
drop_na() %>%
distinct() %>%
count(); nperfis_analisados
nerros <- nrow(erro); nerros
percent_erros <- percent(nerros/nperfis_analisados$n[1], 0.1); percent_erros
nposts_erro <- nrow(erros_twint); nposts_erro
percent_posts_erro <- percent(nposts_erro/ntweets_analisados); percent_posts_erro
primeiro_dia <- min(full_data$date); primeiro_dia
ultimo_dia <- max(full_data$date); ultimo_dia
nbots <- length(unique(bots$user_id)); nbots
nbots_twint <- length(unique(bots_twint$user_id)); nbots_twint
percent_bots <- percent(nbots/nperfis_analisados$n, 0.1); percent_bots
nposts_bots <- nrow(bots_twint); nposts_bots
ntweets_bots_twint <- bots_twint %>% filter(retweet == 'False') %>% nrow(); ntweets_bots_twint
nretweets_bots_twint <- bots_twint %>% filter(retweet == 'True') %>% nrow(); nretweets_bots_twint
percent_posts_bots <- percent(nposts_bots/ntweets_analisados, 0.1); percent_posts_bots
percent_tweets_bots <- percent(ntweets_bots_twint/nposts_bots); percent_tweets_bots
percent_retweets_bots <- percent(nretweets_bots_twint/nposts_bots); percent_retweets_bots
total_horas <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "h")),); total_horas
total_dias <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "d")),); total_dias
total_semanas <- round(as.numeric(difftime(ultimo_dia, primeiro_dia, units = "w")),); total_semanas
stats_df <- t(data.frame('Total de tweets' = ntweets_analisados,
'Total_perfis analisados' = nperfis_analisados,
'Total perfis não analisados' = nerros,
'Porc. perfis com erro' = percent_erros,
'Inicio' = primeiro_dia,
'Fim' = ultimo_dia,
'Total de horas' = total_horas,
'Total de dias' = total_dias,
'Total de semanas' = total_semanas,
'Total perfis automatizados' = nbots,
'Porc. Total perfis automatizados' = percent_bots,
'Posts de perfis automatizados' = nposts_bots,
'Porc. Posts de perfis automatizados' = percent_posts_bots))
output_file <- "resumo.xlsx"
# Salvar arquivo
write.xlsx2(stats_df, output_file, sheetName = "Resumo",
col.names = F, row.names = TRUE, append = TRUE)
ntweets <- nrow(full_data[full_data$retweet == 'False',]); ntweets
percent_rts <- percent(nretweets/ntweets_analisados, 0.1); percent_rts
percent_tweets <- percent(ntweets/ntweets_analisados, 0.1); percent_tweets
# Plot
tweets_rts <- c(`RT` = nretweets, `Tweets` = ntweets)
nretweets <- nrow(full_data[full_data$retweet == 'True',]); nretweets
ntweets <- nrow(full_data[full_data$retweet == 'False',]); ntweets
percent_rts <- percent(nretweets/ntweets_analisados, 0.1); percent_rts
percent_tweets <- percent(ntweets/ntweets_analisados, 0.1); percent_tweets
# Plot
tweets_rts <- c(`RT` = nretweets, `Tweets` = ntweets)
waffle(title = 'Proporção de RTs e Tweets compartilhados pelos usuários',
tweets_rts / 1000, rows = 6,
colors = c(paleta_its[2], paleta_its[6]),
xlab = 'cada unidade = 10000 registros')
plot_tweets_rts <- waffle(
tweets_rts / 1000, rows = 6,
colors = c(paleta_its[2], paleta_its[6]),
#use_glyph = 'twitter',
#glyph_size = 7,
#glyph_font = "Font Awesome 5 Brands Regular",
#glyph_font_family = "FontAwesome5Brands-Regular",
legend_pos="bottom-left") +
labs(title = "Proporção de <b style='color:#8a8a8a'>tweets</b> e <b style='color:#54318C'>RTs</b> ",
subtitle = "<span style='color:#6c6c6c'>cada unidade = 1.000 registros</span>") +
theme(text = element_text(family = "sans"),
plot.title = element_markdown(lineheight = 1.1, size = 20),
plot.subtitle = element_markdown(lineheight = 1.1, size = 16),
axis.title.x = element_text(size = 13, vjust = 1,
hjust = 0, colour = paleta_its[7]),
plot.margin = unit(c(0,0,0,0),"mm")
); plot_tweets_rts
ggsave('output/plot0_tweets_rts.png', width = 7, height = 5, units = 'in', dpi = 100)
## 3. Analise Temporal #####
full_data_bckp <- full_data
# Adicionar intervalo de 7 dias
full_data$interval_7day <- cut(as.POSIXct(full_data$date), breaks = "7 days",)
# Adicionar intervalo de 1 dia
full_data$interval_1day <- cut(as.POSIXct(full_data$date), breaks = "1 day",)
# Adicionar intervalo de 1 h
full_data$interval_1hour <- cut(as.POSIXct(full_data$date), breaks = "1 hour",)
# Adicionar intervalo de 2 h
full_data$interval_2hour <- cut(as.POSIXct(full_data$date), breaks = "2 hour",)
# Adicionar intervalo de 6 h
full_data$interval_6hour <- cut(as.POSIXct(full_data$date), breaks = "6 hour",)
# Adicionar intervalo de 3 h
full_data$interval_3hour <- cut(as.POSIXct(full_data$date), breaks = "3 hour",)
# Adicionar intervalo de 10 min
full_data$interval_10min <- cut(as.POSIXct(full_data$date), breaks = "10 min",)
# Adicionar intervalo de 5 min
full_data$interval_5min <- cut(as.POSIXct(full_data$date), breaks = "5 min")
View(full_data)
# Para agilizar 2:
write_csv(full_data, 'data/processed/full_data_details.csv')
full_data <- read_csv('data/processed/full_data_details.csv', col_types = cols(.default = "c"))
# Por dia
ft_temporal <- plyr::count(full_data, 'interval_3hour')
View(ft_temporal)
ft_temporal$interval <- format(strptime(ft_temporal$interval_3hour,
"%Y-%m-%d %H:%M"),
format = "%d/%m (%Hh)")
ft_temporal_outline <- boxplot.stats(ft_temporal$freq)$out
peaks <- ft_temporal %>%
filter(freq %in% ft_temporal_outline); peaks; str(peaks) #%>% droplevels()
# Plot temporal
plot_temporal <- ggplot(ft_temporal, aes(x = fct_reorder(interval, as.Date(interval_3hour)), y = freq)) +
geom_line(group = 1, color = paleta_its[2], size = 1) +
#geom_point(data = peaks, aes(x = interval, y = freq),
#           color = paleta_its[7], size = 7, alpha = .4) +
labs(title = "Evolução do compartilhamento dos tweets",
subtitle = "publicados entre os dias 23/08 e 02/09 de 2022",
x = "",
y = "") +
scale_x_discrete(guide = guide_axis(check.overlap = TRUE),
breaks = ft_temporal$interval[seq(1, nrow(ft_temporal), by = 4)]) +
theme_minimal() +
scale_y_continuous(breaks = seq(0, max(ft_temporal$freq) + 10000, 2000),
name = "Nº de tweets") +
theme(text = element_text(family = "sans"),
plot.title = element_text(size = 13,
colour = paleta_its[8]),
plot.subtitle = element_text(size = 12,
colour = paleta_its[8]
),
axis.text.x = element_text(size = 12,
colour = paleta_its[7]),
axis.text.y = element_text(size = 12,
colour = paleta_its[7]),
axis.title.y = element_text(size = 12,
colour = paleta_its[7],
vjust = 2,
hjust = 0.98),
plot.margin = unit(c(.5,2,.5,1),"cm"),
panel.grid.minor.y = element_blank()); plot_temporal
ggsave('output/plot1_temporal.png', width = 18, height = 10, units = 'in', dpi = 100)
write.xlsx2(ft_temporal, output_file, sheetName = "EvolucaoTemporal",
col.names = F, row.names = TRUE, append = TRUE)
percent_peaks_tweets <- percent(length(filter_tweets_peaks$id)/ntweets_analisados); percent_peaks_tweets
# Análise dos picos de compartilhamento
filter_tweets_peaks <- full_data %>%
filter(retweet == 'False') %>%
filter(interval_1day == '2022-09-01') %>%
arrange(desc(as.numeric(nretweets)))
percent_peaks_tweets <- percent(length(filter_tweets_peaks$id)/ntweets_analisados); percent_peaks_tweets
#percent_peaks_tweets <- percent(sum(peaks$freq)/ntweets_analisados); percent_peaks_tweets
percent_peaks_days <- percent(nrow(peaks)/total_dias); percent_peaks_days
# Separando o que é só tweet
filter_tweets_peaks_perday <- filter_tweets_peaks %>%
filter(retweet == 'False') %>%
arrange(-as.numeric(nretweets))
# Plot dias de pico ###
peaks$interval_1day <- levels(droplevels(peaks$interval_1day))
View(peaks)
View(filter_tweets_peaks)
View(filter_tweets_peaks_perday)
peaks$interval <- levels(droplevels(peaks$interval))
View(peaks)
